{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Team Task (Permutated MNIST)\n",
    "## LoÃ¯c Rosset, Nanae Aubry, Kilian Ruchti, Lionel Ieri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_task2c import PR_CNN\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "TRAIN_DATASET = \"../dataset/mnist-permutated-png-format/mnist/train\"\n",
    "VALIDATION_DATASET = \"../dataset/mnist-permutated-png-format/mnist/val\"\n",
    "TEST_DATASET = \"../dataset/mnist-permutated-png-format/mnist/test\"\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DATASET, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=VALIDATION_DATASET, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=TEST_DATASET, transform=transform)\n",
    "\n",
    "params = {'batch_size': 64, 'shuffle': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, **params)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print('images shape on PyTroch : ', images.size())\n",
    "print('labels shape on PyTroch : ', labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(images[:25], nrow=5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(nb_epochs, train, val, step):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(nb_epochs), train)\n",
    "    plt.plot(np.arange(nb_epochs), val)\n",
    "    plt.legend(['training', 'validation'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(f'{step} value')\n",
    "    plt.title(f'Train/val {step}');\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training():\n",
    "    \n",
    "    def __init__(self, nb_epochs=20, device=torch.device('cpu')):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.device = device\n",
    "    \n",
    "    def train(self, model, train_loader, optimizer, loss_func):\n",
    "        model.train()\n",
    "        \n",
    "        losses = []\n",
    "        correct_train_pred = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data = data.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Predict the classes of the model\n",
    "            output = model(data)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_func(output, labels)\n",
    "            \n",
    "            # Perform backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save current loss\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Save the number of correct classified items\n",
    "            predicted_labels = output.argmax(dim=1)\n",
    "            nb_correct = (predicted_labels == labels).sum().item()\n",
    "            correct_train_pred += nb_correct\n",
    "    \n",
    "        train_accuracy = 100. * (correct_train_pred / len(train_loader.dataset))\n",
    "        \n",
    "        return np.mean(np.array(losses)), train_accuracy\n",
    "    \n",
    "    def validation(self, model, val_loader, loss_func):\n",
    "        model.eval()\n",
    "        \n",
    "        losses = []\n",
    "        correct_val_predictions = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                output = model(data)\n",
    "                \n",
    "                loss = loss_func(output, labels)\n",
    "                \n",
    "                # Save current loss\n",
    "                losses.append(loss.item())\n",
    "    \n",
    "                # Save the number of correct classified items\n",
    "                predicted_labels = output.argmax(dim=1)\n",
    "                n_correct = (predicted_labels == labels).sum().item()\n",
    "                correct_val_predictions += n_correct\n",
    "                \n",
    "        val_accuracy = 100. * (correct_val_predictions / len(val_loader.dataset))\n",
    "                \n",
    "        return np.mean(np.array(losses)), val_accuracy\n",
    "\n",
    "    def _print_info(self, train_loss, val_loss, train_acc, val_acc):\n",
    "        print(f'Train_loss: {train_loss:.3f} |\\\n",
    "                Val_loss: {val_loss:.3f} |\\\n",
    "                Train_acc: {train_acc:.3f} |\\\n",
    "                Val_acc: {val_acc:.3f}')\n",
    "\n",
    "    def fit(self, model, train_loader, val_loader, optimizer, loss_func):\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "\n",
    "        for epoch in range(self.nb_epochs):\n",
    "            train_loss, train_acc = self.train(model, train_loader, optimizer, loss_func)\n",
    "            val_loss, val_acc = self.validation(model, val_loader, loss_func)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "        self._print_info(train_losses[-1], val_losses[-1], train_accuracies[-1], val_accuracies[-1])\n",
    "\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_rate_set = [0.001, 0.01, 0.1]\n",
    "nb_epochs = 5\n",
    "trainer = Training(nb_epochs)\n",
    "\n",
    "# best parameters: (validation acc, learning rate)\n",
    "best_parameters = (float(\"-INF\"), None, None)\n",
    "\n",
    "# keep a copy of the best trained network\n",
    "best_model = None\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for l_rate in l_rate_set:\n",
    "    cnn = PR_CNN()\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), l_rate)\n",
    "    \n",
    "    # The training take place here :\n",
    "    print(f'learning rate: {l_rate}')\n",
    "    stats_training = trainer.fit(cnn, train_loader, val_loader, optimizer, loss_function)\n",
    "    \n",
    "    if stats_training[3][-1] > best_parameters[0]:\n",
    "        best_parameters = (stats_training[3][-1], l_rate, stats_training)\n",
    "        best_model = deepcopy(cnn)\n",
    "\n",
    "best_acc, best_l_rate, best_stats = best_parameters\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = best_stats\n",
    "\n",
    "print(f'\\n\\nBest parameters: {best_l_rate} learning_rate and validation accuracies: {best_acc:.2f}%')\n",
    "\n",
    "plot_graph(nb_epochs, train_losses, val_losses, \"Loss\")\n",
    "plot_graph(nb_epochs, train_accuracies, val_accuracies, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_loss_test, cnn_acc_test = trainer.validation(best_model, test_loader, loss_function)\n",
    "print(f'Accuracy on the test dataset {cnn_acc_test:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is 3 filters!\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2352, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 30)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the images\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Create the layer\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data\n",
    "l_rate_set = [0.001, 0.01, 0.1]\n",
    "nb_epochs = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Training(nb_epochs)\n",
    "\n",
    "# best parameters: (validation acc, learning rate)\n",
    "best_parameters = (float(\"-INF\"), None, None)\n",
    "\n",
    "# keep a copy of the best trained network\n",
    "best_model = None\n",
    "\n",
    "for l_rate in l_rate_set:\n",
    "    mlp = MLP()\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=l_rate)\n",
    "    \n",
    "    # The training take place here :\n",
    "    print(f'learning rate: {l_rate}')\n",
    "    stats_training = trainer.fit(mlp, train_loader, val_loader, optimizer, loss_function)\n",
    "    \n",
    "    if stats_training[3][-1] > best_parameters[0]:\n",
    "        best_parameters = (stats_training[3][-1], l_rate, stats_training)\n",
    "        best_model = deepcopy(mlp)\n",
    "\n",
    "best_acc, best_l_rate, best_stats = best_parameters\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = best_stats\n",
    "\n",
    "print(f'\\n\\nBest parameters: {best_l_rate} learning_rate and validation accuracies: {best_acc:.2f}%')\n",
    "\n",
    "plot_graph(nb_epochs, train_losses, val_losses, \"Loss\")\n",
    "plot_graph(nb_epochs, train_accuracies, val_accuracies, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_loss_test, mlp_acc_test = trainer.validation(best_model, test_loader, loss_function)\n",
    "print(f'Accuracy on the test dataset {mlp_acc_test:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Comparison with normal MNIST (results from 2b and 2c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
